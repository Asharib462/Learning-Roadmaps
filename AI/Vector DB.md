# Vector Databases: A Comprehensive Guide

What is vector?
- Thought for a couple of seconds


In a vector database, a **vector** is simply a fixed‚Äêlength array of numbers (floats) that represents some piece of data‚Äîusually the semantic content of text, an image, audio clip, etc.‚Äîin a high-dimensional space. Key points:

* **Embedding**: You generate a vector (often 128‚Äì1,024 dimensions) by running your data through a machine-learning model (e.g. a transformer or CNN).
* **Coordinates**: Each position in that array encodes one ‚Äúfeature‚Äù of the data. For text, those features capture meaning; for images, visual patterns.
* **Similarity Search**: Vectors allow you to measure ‚Äúcloseness‚Äù (via cosine similarity, Euclidean distance, etc.), so you can ask ‚Äúfind items most similar to this query vector‚Äù very efficiently.
* **Indexing**: Vector databases build specialized indexes (e.g. HNSW, IVF, PQ) over those high-dimensional points so that nearest-neighbor queries run in milliseconds‚Äîeven over millions or billions of vectors.

**Example**

```js
// A 5-dimensional toy ‚Äúembedding‚Äù for the sentence ‚Äúhello world‚Äù
const vector = [0.12, -0.03, 0.87, 0.44, -0.21];
```

You‚Äôd store `vector` alongside identifiers or metadata in your DB. When you query with another vector‚Äîsay, representing ‚Äúgreetings planet‚Äù‚Äîthe database quickly finds stored vectors whose geometric distance to your query vector is smallest. That‚Äôs the essence of how vector search powers recommendations, semantic search, and RAG pipelines.


## 1. Introduction to Vector Databases

### Definition
- A specialized database for storing high-dimensional vectors
- Used to store embeddings (word embeddings, image embeddings)
- Designed to handle complex data types (documents, images, PDFs)

### Why Vector Databases?
- Traditional databases (MySQL, etc.) are not suitable for high-dimensional vector data
- Provides specialized capabilities for vector storage and querying
- Essential for modern AI/ML applications

## 2. How Vector Databases Work

### Core Functionality
- Converts high-dimensional data into vector representations
- Stores these vectors efficiently
- Enables efficient querying of vector data

## 3. Applications and Use Cases

### Practical Applications
- Integration with Large Language Models (LLMs)
- Enables various AI/ML tasks
- Modern AI application development

## 4. Popular Vector Database Options

### Available Solutions
- ChromaDB
- Pinecone
- Weaviate

## 5. Implementation

### Technical Details
- Python implementation
- LangChain framework integration
- End-to-end vector data management solutions

## 6. Practical Implementation Topics
- Converting data to vectors
- Storing vectors in vector databases
- Performing tasks using LLMs with vector databases


## Vector Database: Overview and Importance

### Why We Need Vector Databases?

* Traditional relational databases (e.g., MySQL, PostgreSQL, SQLite) are ideal for structured data like CSV or Excel sheets.
* **Problem**: They are inefficient for storing and querying **unstructured data** (images, text, audio, video), which make up 80-85% of total data.
* **Reason**: Unstructured data lacks predefined schemas, making it difficult to store and retrieve based on relational queries.

### Limitations of Relational Databases with Unstructured Data

* To store images in traditional databases, manual labeling and schema creation are required (e.g., Animal, Color, Tags columns).
* Manually labeling data (e.g., tagging images as "dog," "golden," "cute," etc.) is:

  * Inefficient.
  * Error-prone.
  * Time-consuming.
* Converting images to base64 strings allows storage but makes similarity searches impossible without manual tagging.

---

## Solution: Vector Embeddings

### What Are Vector Embeddings?

* **Vector embeddings** are numerical representations of unstructured data (image, text, audio, video) created by neural network-based embedding models.
* Embeddings capture semantic meaning, enabling computers to understand context and similarity.

### How Embedding Models Work

1. **Input**: Unstructured data (e.g., text: "King," "Queen," "Man," "Woman," "Monkey").
2. **Embedding Model**: Uses neural networks (e.g., transformers, LLM-based models) to extract features (attributes like gender, wealth, power, etc.) automatically.
3. **Output**: Numerical vectors that represent input data in a high-dimensional space.

### Example: Vector Generation from Text

| Word   | Gender | Wealth | Power | Weight | Speak |
| ------ | ------ | ------ | ----- | ------ | ----- |
| King   | 1 (M)  | 1      | 1     | 0.8    | 1     |
| Queen  | 0 (F)  | 1      | 0.7   | 0.5    | 1     |
| Man    | 1 (M)  | 0.5    | 0.5   | 0.7    | 1     |
| Woman  | 0 (F)  | 0.3    | 0.2   | 0.5    | 1     |
| Monkey | 1 (M)  | 0      | 0     | 0.3    | 0     |

* Each word becomes a numeric vector (e.g., **King** = `[1, 1, 1, 0.8, 1]`).
* Similar words ("King," "Queen") cluster close together in vector space, enabling effective similarity searches.

### Visual Example of Vector Clusters

* Similar items form clusters when plotted in multidimensional spaces:

  * **Cluster 1**: King, Queen (similar semantic context)
  * **Cluster 2**: Man, Woman (human context)
  * **Cluster 3**: Monkey (distinct from humans)

### Embedding Models Examples

* **Word2Vec**
* **Transformer-based embeddings**
* **Large Language Models (LLMs)**:

  * OpenAI embeddings (GPT-3/GPT-4) *(Paid)*
  * Hugging Face embeddings *(Free/Open Source)*
  * Facebook LLaMA embeddings *(Free/Open Source with access request)*
  * Google PaLM embeddings *(Free/Open Source)*

---

## Vector Databases and Similarity Search

### Storing Embeddings in Vector Databases

* Vector databases specifically store embedding vectors for fast retrieval and similarity searches.
* Process:

  1. Convert unstructured data into embedding vectors.
  2. Store vectors in a vector database.

### Similarity Search

* Vector databases use similarity search to retrieve vectors that closely match a query.
* Common similarity metrics:

  * **Euclidean distance**
  * **Manhattan distance**
  * **Cosine similarity** (widely used)

---

## Indexing in Vector Databases

### Problem with Basic Similarity Search

* Basic similarity searches without indexing are computationally expensive (searches every vector individually).

### Solution: Vector Indexes

* **Vector indexes** are specialized data structures that:

  * Group similar vectors into clusters.
  * Accelerate searches by limiting the search space.
* When querying, the index quickly identifies relevant clusters, greatly speeding up retrieval.

### Indexing Example:

* **Step 1**: Data vectors are grouped into clusters (indexed).
* **Step 2**: A new query searches only the relevant cluster, returning results faster than searching every vector individually.

---

## Summary

* **Traditional databases** are not ideal for unstructured data due to manual schema creation and labeling inefficiencies.
* **Vector databases** overcome this by storing numerical vector representations (**embeddings**) of data.
* **Embedding models** translate data into vectors based on learned features, enabling semantic queries and similarity searches.
* **Vector indexing** significantly optimizes the speed of similarity queries by clustering data.

---

## Use Cases of Vector Databases

### 1. Long-Term Memory for Large Language Models (LLMs)

* Vector databases act as persistent storage for embeddings.
* They provide **long-term memory** for LLMs by storing semantic embeddings.
* Useful for applications needing to recall context over extended interactions.

### 2. Semantic Search

* Vector databases perform searches based on **semantic meaning** rather than keywords alone.
* Example: Searching "King" retrieves related concepts like "Queen," based on contextual similarity.

### 3. Similarity Search (Multimodal)

* Supports similarity searches for:

  * **Text:** Finding similar phrases or documents.
  * **Images:** Retrieving visually similar images (e.g., similar dog images).
  * **Videos and Audio:** Finding similar audio clips or videos.

### 4. Recommendation Systems

* Effective for content recommendation systems.
* Provides recommendations by finding items with similar vector embeddings.
* Example: If a user interacts with food-related content, similar food-related content can be suggested.

---

## Popular Vector Databases

### Commonly Used Vector Databases:

* **Chroma DB**

  * Local vector database.
  * Open source, suitable for quick prototyping.

* **Pinecone**

  * Cloud-based, scalable solution.
  * Subscription required for commercial use (limited free tier).

* **Weaviate**

  * Cloud-based, suitable for scalable deployments.
  * Subscription-based with a limited free tier for learning.

* **Redis**

  * Flexible, high-performance option for embeddings.

* **FAISS**

  * Developed by Facebook Research.
  * Local and highly optimized for fast similarity search.

---

## Practical Usage

* **Chroma DB, Pinecone, and Weaviate** are extensively used.
* Recommended to learn these three databases to build most generative AI applications.
* Chroma DB is commonly used locally, ideal for small projects or learning.
* Pinecone and Weaviate are preferred for cloud scalability and commercial deployments.

---

**Next Steps for Learning:**

* Hands-on practice with Chroma DB, Pinecone, and Weaviate.
* Learn integration with Python, particularly using frameworks like LangChain.
* Understand setup processes, querying methods, and practical implementation examples.

---

## ‚úÖ Practical Implementation of ChromaDB (Local Vector Database)

### üìå What is ChromaDB?

* A **local vector database** designed for storing and retrieving vector embeddings.
* Currently **does not support cloud-hosted clusters**, but a hosted version is in development.
* **Open-source** and supports **Python** and **JavaScript** integration.
* Compatible with **LangChain** and **LlamaIndex** frameworks.

---

## üöÄ Setup Steps

### 1. **Environment Setup**

* Recommended to use **Google Colab** (offers free GPU access).
* Required Python packages (install via pip):

  ```bash
  pip install chromadb openai langchain tiktoken
  ```

### 2. **OpenAI API Key**

* Create an API key from [https://platform.openai.com/account/api-keys](https://platform.openai.com/account/api-keys)
* Store securely in the environment variable:

  ```python
  import os
  os.environ["OPENAI_API_KEY"] = "your_api_key_here"
  ```

---

## üìÇ Data Loading and Preparation

### 3. **Load Text Data**

* Download text articles (`.txt`) from Dropbox.
* Unzip and organize in a folder (`new_articles/`).
* Use `DirectoryLoader` and `TextLoader` from `langchain.document_loaders`:

  ```python
  from langchain.document_loaders import DirectoryLoader, TextLoader
  loader = DirectoryLoader("new_articles", glob="*.txt", loader_cls=TextLoader)
  documents = loader.load()
  ```

---

## ‚úÇÔ∏è Chunking the Data

### 4. **Why Chunking?**

* LLMs like **GPT-3.5 Turbo** have token limits (e.g., \~4097 tokens).
* Splitting ensures chunks fit within input limits.

### 5. **Chunk Settings**

* Use `RecursiveCharacterTextSplitter`:

  ```python
  from langchain.text_splitter import RecursiveCharacterTextSplitter
  text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
  texts = text_splitter.split_documents(documents)
  ```

---

## üîÅ Embedding and Storing in ChromaDB

### 6. **Embedding Model Setup**

* Use OpenAI‚Äôs embedding model: `text-embedding-ada-002`.

  ```python
  from langchain.embeddings import OpenAIEmbeddings
  embedding = OpenAIEmbeddings()
  ```

### 7. **Create and Save ChromaDB**

* Save embeddings into ChromaDB locally:

  ```python
  from langchain.vectorstores import Chroma
  vector_db = Chroma.from_documents(texts, embedding, persist_directory="db")
  vector_db.persist()
  ```

---

## üîç Vector Retrieval (Similarity Search)

### 8. **Load Existing ChromaDB**

```python
vector_db = Chroma(persist_directory="db", embedding_function=embedding)
```

### 9. **Retrieve Similar Documents**

```python
retriever = vector_db.as_retriever()
results = retriever.get_relevant_documents("How much money did Microsoft raise?")
```

---

## ü§ñ Integrating with LLM (Question Answering)

### 10. **Build QA Chain with LangChain**

* Uses `RetrievalQA` to query vectors and interpret with GPT:

  ```python
  from langchain.chains import RetrievalQA
  from langchain.llms import OpenAI

  llm = OpenAI()
  qa_chain = RetrievalQA.from_chain_type(llm=llm, retriever=retriever, return_source_documents=True)
  ```

### 11. **Ask a Question**

```python
query = "How much money did Microsoft raise?"
response = qa_chain.run(query)
print(response)
```

---

## üßπ Cleanup and Backup

### 12. **Backup & Delete ChromaDB**

* Zip the database folder:

  ```bash
  !zip -r db.zip db
  ```
* Delete the folder:

  ```bash
  !rm -rf db
  ```

### 13. **Restore ChromaDB**

* Unzip the backup and reload:

  ```bash
  !unzip db.zip
  ```

---

## üìù Key Concepts Recap

| Concept                | Description                                                 |
| ---------------------- | ----------------------------------------------------------- |
| **Embedding**          | Numerical representation of text/images/audio using a model |
| **ChromaDB**           | Local DB for storing vector embeddings                      |
| **Retriever**          | Searches similar vectors using cosine similarity            |
| **LLM (GPT-3.5)**      | Converts chunks into contextual answers                     |
| **LangChain**          | Framework to build chains for LLM workflows                 |
| **Chunk Size/Overlap** | Helps split large text while retaining context              |

---

### üß≠ Overview

* **Pinecone** is a **cloud-hosted vector database** that allows storing and querying high-dimensional vectors (e.g., from text embeddings).
* Ideal for scalable, production-level use cases.

---

### üõ†Ô∏è Setup

* Install necessary packages:

  * `langchain`, `pinecone-client`, `openai`, `tiktoken`, `pypdf`.
* Set OpenAI API key and Pinecone environment variables (API key, environment, index name).
* Create an index on Pinecone dashboard:

  * Define vector dimension (e.g., 1536 for OpenAI).
  * Use cosine similarity.

---

### üìÑ Load & Prepare Data

* PDF files downloaded from Google Drive using `gdown`.
* Text extracted using `PyPDFLoader`.
* Text chunked with `RecursiveCharacterTextSplitter`:

  * E.g., chunk size = 1000, overlap = 200.

---

### üß† Embedding & Upload

* Text chunks embedded using **OpenAI embeddings** (dimension: 1536).
* Stored in Pinecone index using `Pinecone.from_texts`.

---

### üîç Querying

* Perform **similarity search** using `.similarity_search(query, k=N)`.
* Returns top-N most relevant text chunks from the vector store.

---

### ü§ñ Use LLM for Answers

* Use **LangChain's RetrievalQA chain** to:

  * Pass retrieved chunks + query to an LLM (GPT-3.5 Turbo).
  * Get concise, human-readable answers.

---

### üí¨ Interactive QA Loop

* A `while` loop allows user to input questions continuously.
* Type "exit" to terminate.

---

### üßπ Cleanup

* View and delete the Pinecone index via dashboard.
* Indexes auto-expire on free tier after \~7 days.

---

### ‚úÖ Final Thoughts

* Pinecone offers an intuitive interface and visual inspection of vectors.
* Easier and more beginner-friendly than alternatives like Weaviate.
* Good choice for deploying production-grade semantic search systems.

---

## üåê **Weaviate Vector Database - Summary & Workflow**

---

### üîß Setup & Installation

* **Libraries Installed**:

  * `weaviate-client`
  * `openai`, `langchain`, `unstructured` (for parsing complex PDFs)
* Required Credentials:

  * OpenAI API key (for embeddings)
  * Weaviate API key
  * Weaviate Cluster URL

---

### üõ†Ô∏è Account & Cluster Setup

* Create a free **Weaviate cloud account**.
* Launch a **new cluster** (free tier lasts 21 days).
* Retrieve:

  * Cluster URL
  * API Key

---

### üìÑ Load & Prepare Data

* Upload a **PDF file** (e.g., YOLOv7 research paper).
* If it's a slide or image-heavy PDF, install and use `unstructured` for accurate text extraction.
* Extracted text is then **chunked** using `RecursiveCharacterTextSplitter`.

---

### üß† Generate Embeddings

* Use **OpenAI Embeddings** (`text-embedding-ada-002`) for each chunk.
* Embedding dimension: **1536**
* API key can be passed directly or as an environment variable.

---

### üóÉÔ∏è Initialize Weaviate Client & Schema

* Authenticate using API key and cluster URL.
* **Schema setup is required** in Weaviate (unlike Pinecone):

  * Define class name, description, and embedding model.
* Schema tells Weaviate how to structure and store vectors.

---

### üíæ Store Embeddings

* Store vector representations of your document chunks in Weaviate using LangChain‚Äôs integration.
* After a short delay (\~5 mins), updates appear in the Weaviate dashboard.

---

### üîç Querying & Similarity Search

* Perform **similarity search** using `.similarity_search(query, k=20)` on stored vector database.
* Returns top-k similar chunks.

---

### ü§ñ Use LLM to Generate Answers

* Use **LangChain‚Äôs RetrievalQA or QA Chain** with an OpenAI LLM.
* Input: user query + retrieved chunks.
* Output: synthesized answer.

---

### üß™ Example Queries

* Query: *‚ÄúWhat is YOLO?‚Äù*
* Output: Describes YOLO as a real-time object detection model with authorship and publication info.

---

### üîÅ Optional While Loop

* A loop can be created for continuous QA interaction.
* Type ‚Äúexit‚Äù to stop.

---

### üß† Observations & Final Notes

* **Weaviate requires more setup (schema definition)** than Pinecone.
* **Supports multiple languages (Python, Java, JS, Go)**.
* Weaviate provides **flexible schema control**, but Pinecone is **easier for quick prototypes**.
* Both allow integration with OpenAI embeddings and LangChain chains.

---

### ‚úÖ Conclusion

* You now know how to:

  * Set up Weaviate
  * Load and chunk documents
  * Generate embeddings
  * Store vectors
  * Retrieve and query using LLMs




### Research more on this 

### Resources/Tools used:
**Youtube Video:** https://www.youtube.com/watch/8KrTO9bS91s
**Transcript Generator:** https://tactiq.io/tools/youtube-transcript
